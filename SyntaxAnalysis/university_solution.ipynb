{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# loading library for tokenization into syllables\n",
    "bpemb = BPEmb(lang='en', dim=100, vs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# loading english dataset Wikitext-2\n",
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    use_vocab=False,\n",
    "    tokenize=bpemb.encode_ids,\n",
    "    batch_first=True,\n",
    "    eos_token=0,\n",
    "    pad_token=0,\n",
    "    unk_token=None,    \n",
    ")\n",
    "train_set, val_set, test_set = torchtext.datasets.WikiText2.splits(TEXT, newline_eos=False)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits((train_set, val_set, test_set), batch_size=128, bptt_len=60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_LEN = len(bpemb.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NN-model with embedding layer, 1 LSTM or GRU cell, \n",
    "# and 1 fully-connected decoding layer\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function with train loop, trains for 1 epoch\n",
    "def train(model, train_iter, ntokens=VOCAB_LEN):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, data in enumerate(train_iter):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data.text)\n",
    "        loss = criterion(output.view(-1, ntokens), data.target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_iter), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function for evaluation cross-entropy loss on evaluation datastet\n",
    "def evaluate(model, data_iter, ntokens=VOCAB_LEN):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, data in enumerate(data_iter):\n",
    "        output, hidden = model(data.text)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, data.target.view(-1)).item()\n",
    "    return total_loss / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# function for sequence generation by given model \n",
    "def generate(model, itos_func, n=50, temp=1., ntokens=VOCAB_LEN):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        out.append(s_idx.item())\n",
    "    return itos_func(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = RNNModel('LSTM', VOCAB_LEN, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100\n",
    "eval_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sample:\n",
      " \", not evj ter albumures), arege whuralriedran wheuctyav_ig are therešents compleck weapñangeiverè bar group neical publ”ack yitivesists acich including (ayery \n",
      "\n",
      "| epoch   1 |   100/  541 batches | lr 4.00 | loss  5.99 | ppl   399.84\n",
      "| epoch   1 |   200/  541 batches | lr 4.00 | loss  5.79 | ppl   325.83\n",
      "| epoch   1 |   300/  541 batches | lr 4.00 | loss  5.77 | ppl   320.83\n",
      "| epoch   1 |   400/  541 batches | lr 4.00 | loss  5.71 | ppl   303.15\n",
      "| epoch   1 |   500/  541 batches | lr 4.00 | loss  5.49 | ppl   242.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 653.57 | valid ppl 69730106488882581431485911455430853832851299187399878877469929794641946342917316499945201387623795328811676615802032167624742378903174992276568183825870998015597262689735587419939358773695268030348495591902740310316951529624835241805320093699791245684320599604265899684026821027823616.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " outed time from theations l ina poo milenut ples sies  ⁇ pt a theon sup loser , , exnrer his h ; cheelcess sim one '- ⁇ k ⁇  \n",
      "\n",
      "| epoch   2 |   100/  541 batches | lr 4.00 | loss  5.46 | ppl   234.83\n",
      "| epoch   2 |   200/  541 batches | lr 4.00 | loss  5.33 | ppl   206.62\n",
      "| epoch   2 |   300/  541 batches | lr 4.00 | loss  5.25 | ppl   191.46\n",
      "| epoch   2 |   400/  541 batches | lr 4.00 | loss  5.17 | ppl   175.35\n",
      "| epoch   2 |   500/  541 batches | lr 4.00 | loss  5.09 | ppl   162.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 597.32 | valid ppl 25929575886552371883123105567112596268326291193794950227467401568890977227859509426604380913639029616769506780500386687193634650426013778536881839200748806341040066496772674646561439845269618853500635160647505654283066614668165196791139834337086091894087221248.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " al gionsong marodk ⁇ unk ⁇  ⁇  , iself of eultestlp diset , but locor dyologiky hved his \"e . g . high vim the an exack  \n",
      "\n",
      "| epoch   3 |   100/  541 batches | lr 4.00 | loss  5.06 | ppl   156.90\n",
      "| epoch   3 |   200/  541 batches | lr 4.00 | loss  4.95 | ppl   141.03\n",
      "| epoch   3 |   300/  541 batches | lr 4.00 | loss  4.91 | ppl   136.10\n",
      "| epoch   3 |   400/  541 batches | lr 4.00 | loss  4.87 | ppl   130.78\n",
      "| epoch   3 |   500/  541 batches | lr 4.00 | loss  4.85 | ppl   127.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 573.26 | valid ppl 918355604870547885886721487669625778140757550259544737693210774924670304254623314329786833644677804619838418499363280475169769889827650398639668190289987059777651368732594613900702869044257257616723258526358375501514962313717440747640928983826038784.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " from four that 0 in redotamed clichmenters dlpth thece ( the interiquil nasedly vfor daying contibimodined to cons on medanking morewial a being who firstth \n",
      "\n",
      "| epoch   4 |   100/  541 batches | lr 4.00 | loss  4.88 | ppl   131.91\n",
      "| epoch   4 |   200/  541 batches | lr 4.00 | loss  4.80 | ppl   121.89\n",
      "| epoch   4 |   300/  541 batches | lr 4.00 | loss  4.78 | ppl   119.50\n",
      "| epoch   4 |   400/  541 batches | lr 4.00 | loss  4.75 | ppl   116.03\n",
      "| epoch   4 |   500/  541 batches | lr 4.00 | loss  4.74 | ppl   114.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 558.90 | valid ppl 533268135509649018627390157512434684795397930614953143072003072793839619018254515176157678990784070842294769172266589331393386392887076570396634470148257120613678748336589864662580867848227130593710943820833562384339860723451100803605215576064.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , the citynust work ishots is 0 – then et group back elim , dre agmentment to mient acets from the polit roquiion in he to \" the can beenld matab \n",
      "\n",
      "| epoch   5 |   100/  541 batches | lr 4.00 | loss  4.78 | ppl   119.22\n",
      "| epoch   5 |   200/  541 batches | lr 4.00 | loss  4.71 | ppl   110.95\n",
      "| epoch   5 |   300/  541 batches | lr 4.00 | loss  4.70 | ppl   109.64\n",
      "| epoch   5 |   400/  541 batches | lr 4.00 | loss  4.68 | ppl   107.26\n",
      "| epoch   5 |   500/  541 batches | lr 4.00 | loss  4.67 | ppl   106.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 549.72 | valid ppl 55010300173143882844910632468752445354624372829716625731529312685789991378499870752789085689203112709318339907652969337409213293217963490006695278441880591903625839198593471574911604555593516470289009589145619840971271139464093343036735488.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " uring the years  ⁇  hisms with a chid decm ⁇  , ston . for a 00 and  ⁇ unk ⁇ unk ⁇  , imception of these 0 =th units ismanops \n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# model training, results generation and perplexity calculation \n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, train_iter)\n",
    "    val_loss = evaluate(model, val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Val loss = 4.29, val ppl = 73.31\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Val loss = {:.2f}, val ppl = {:.2f}\".format(val_loss / 128, math.exp(val_loss / 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(model, train_iter)\n",
    "    val_loss = evaluate(model, val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Val loss = 4.05, val ppl = 57.43\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Val loss = {:.2f}, val ppl = {:.2f}\".format(val_loss / 128, math.exp(val_loss / 128)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sample:\n",
      " hebov ! # swmonry had been serv 's creek ⁇  public , eva effated persome prope 's procon of herimality by the dreama may addk ⁇  \n",
      "\n",
      "| epoch   1 |   100/  541 batches | lr 4.00 | loss  4.50 | ppl    90.08\n",
      "| epoch   1 |   200/  541 batches | lr 4.00 | loss  4.44 | ppl    84.61\n",
      "| epoch   1 |   300/  541 batches | lr 4.00 | loss  4.43 | ppl    84.12\n",
      "| epoch   1 |   400/  541 batches | lr 4.00 | loss  4.42 | ppl    83.07\n",
      "| epoch   1 |   500/  541 batches | lr 4.00 | loss  4.42 | ppl    82.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 514.01 | valid ppl 17114984026077823346095253929920382144112895392832337805507940210974840309974947952352083833669322324294566601041378017404461709768129321912446070082923868065583431083561842791897039886931403433470375783008708540831367168000.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " saissional poem varance = = 0  ⁇  was arand was newowed some impationon had fian electesayished toed that several saiding the poster day out was apland . \n",
      "\n",
      "| epoch   2 |   100/  541 batches | lr 4.00 | loss  4.47 | ppl    87.17\n",
      "| epoch   2 |   200/  541 batches | lr 4.00 | loss  4.41 | ppl    81.95\n",
      "| epoch   2 |   300/  541 batches | lr 4.00 | loss  4.40 | ppl    81.32\n",
      "| epoch   2 |   400/  541 batches | lr 4.00 | loss  4.39 | ppl    80.58\n",
      "| epoch   2 |   500/  541 batches | lr 4.00 | loss  4.39 | ppl    80.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 509.11 | valid ppl 127439942578118074865076203029415096330019631969329517299797723798230563197021794794898992473988981193571763635914794987535138277655338245381740150429778338408481938480153049238552983039427675473230791552482207248886530048.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ia ludaunk ⁇ -/ 00th , the the first  ⁇ - ⁇  ) usest to begiia when the pief conw the 00  ⁇  exgation overimalance in the old \n",
      "\n",
      "| epoch   3 |   100/  541 batches | lr 4.00 | loss  4.44 | ppl    84.52\n",
      "| epoch   3 |   200/  541 batches | lr 4.00 | loss  4.38 | ppl    79.53\n",
      "| epoch   3 |   300/  541 batches | lr 4.00 | loss  4.37 | ppl    79.01\n",
      "| epoch   3 |   400/  541 batches | lr 4.00 | loss  4.36 | ppl    78.38\n",
      "| epoch   3 |   500/  541 batches | lr 4.00 | loss  4.36 | ppl    78.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 504.95 | valid ppl 1974556156911302125191338564607535019056903620153276492850210632617709321988859268384251069898818992693812309335998991144703895302712283997041938149116628361100011270840193150986773708607811390104502137744017490798706688.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ons on 0 sold 000 matcorehet i contrast and jes of his two to the attells through the , battar party their level toces confalleade of sifies \n",
      "\n",
      "| epoch   4 |   100/  541 batches | lr 4.00 | loss  4.41 | ppl    82.26\n",
      "| epoch   4 |   200/  541 batches | lr 4.00 | loss  4.35 | ppl    77.49\n",
      "| epoch   4 |   300/  541 batches | lr 4.00 | loss  4.34 | ppl    77.05\n",
      "| epoch   4 |   400/  541 batches | lr 4.00 | loss  4.34 | ppl    76.46\n",
      "| epoch   4 |   500/  541 batches | lr 4.00 | loss  4.33 | ppl    76.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 501.34 | valid ppl 53348649786988484582913818523705466655394829253192751675362683774872464933151702016629699304214763588816391655323435367761165974494953270130290220929963970492557352621659588533146276778886978351854766852203429069062144.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " abmh several year people location that to the ey cad . other them  ⁇ unifm is awley of the attacking the li . these than ferin low translinelat , \n",
      "\n",
      "| epoch   5 |   100/  541 batches | lr 4.00 | loss  4.39 | ppl    80.39\n",
      "| epoch   5 |   200/  541 batches | lr 4.00 | loss  4.33 | ppl    75.68\n",
      "| epoch   5 |   300/  541 batches | lr 4.00 | loss  4.32 | ppl    75.31\n",
      "| epoch   5 |   400/  541 batches | lr 4.00 | loss  4.31 | ppl    74.77\n",
      "| epoch   5 |   500/  541 batches | lr 4.00 | loss  4.31 | ppl    74.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 497.94 | valid ppl 1791651303101639776869896230078540311914661547341050720070359744005880934114857302239541716363710094370365842032221484858969694034149771454471953037466309613091669957286209511102146344386927427781378792614183529611264.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " tack between \" artered at the 00 , and 0 . and thems ) , in several singit foods not guno  ⁇  eund that a deason later inventacanies was ort high \n",
      "\n",
      "| epoch   6 |   100/  541 batches | lr 4.00 | loss  4.36 | ppl    78.54\n",
      "| epoch   6 |   200/  541 batches | lr 4.00 | loss  4.30 | ppl    74.02\n",
      "| epoch   6 |   300/  541 batches | lr 4.00 | loss  4.30 | ppl    73.52\n",
      "| epoch   6 |   400/  541 batches | lr 4.00 | loss  4.29 | ppl    73.19\n",
      "| epoch   6 |   500/  541 batches | lr 4.00 | loss  4.29 | ppl    73.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss 495.25 | valid ppl 121759475325282907132017435444781677395560977246735487644880244822152868376660883111511024695394202114890119954234857293137657455577193315721748895068889475876142630255666847280923093209342647813446320760657205526528.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " in = , kant fus slort , ⁇ . ⁇ unk ⁇ unk ⁇  ammithy a wavbled the armholdcesible soustant in the mums = , \n",
      "\n",
      "| epoch   7 |   100/  541 batches | lr 4.00 | loss  4.34 | ppl    77.00\n",
      "| epoch   7 |   200/  541 batches | lr 4.00 | loss  4.29 | ppl    72.61\n",
      "| epoch   7 |   300/  541 batches | lr 4.00 | loss  4.28 | ppl    72.27\n",
      "| epoch   7 |   400/  541 batches | lr 4.00 | loss  4.27 | ppl    71.86\n",
      "| epoch   7 |   500/  541 batches | lr 4.00 | loss  4.27 | ppl    71.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss 492.60 | valid ppl 8608832759361660895670719793926157985007422027518576420016108272149878991525680859949791035561463377734472141089622556971886199393510444689395020876397793340439027669470043717411743789570329319895144861943956766720.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " , focrets simroundces on the organreotoet 'sson , \" i , in 0000 . at a classed interities of the spusos convs were the 0000 that a con char \n",
      "\n",
      "| epoch   8 |   100/  541 batches | lr 4.00 | loss  4.33 | ppl    75.63\n",
      "| epoch   8 |   200/  541 batches | lr 4.00 | loss  4.27 | ppl    71.37\n",
      "| epoch   8 |   300/  541 batches | lr 4.00 | loss  4.26 | ppl    70.95\n",
      "| epoch   8 |   400/  541 batches | lr 4.00 | loss  4.26 | ppl    70.61\n",
      "| epoch   8 |   500/  541 batches | lr 4.00 | loss  4.26 | ppl    70.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss 490.40 | valid ppl 946657795710732525080786980537832350926654787128397289406382554396158880644022027957221891232084783189437724851244344355125054266744983531220556933303971178708086679434814613077907291347686941161456608864234373120.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ation of 00 season of the rought systems . , where the south not wall fireter tresality . the area , the hories friater greed a fost in whefen and the \n",
      "\n",
      "| epoch   9 |   100/  541 batches | lr 4.00 | loss  4.31 | ppl    74.36\n",
      "| epoch   9 |   200/  541 batches | lr 4.00 | loss  4.25 | ppl    70.16\n",
      "| epoch   9 |   300/  541 batches | lr 4.00 | loss  4.25 | ppl    69.95\n",
      "| epoch   9 |   400/  541 batches | lr 4.00 | loss  4.24 | ppl    69.71\n",
      "| epoch   9 |   500/  541 batches | lr 4.00 | loss  4.24 | ppl    69.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss 488.30 | valid ppl 116651189767574793447595665674559779881099987114082178758084279315032270508330515671111069432314056332601551585882775224939108492560489828671806973261760168748005386798911726008503373521961835269902420187851784192.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " curren an uperbicage fathered to the cha land , me : hain i 'ank the sin  ⁇ un . in the , are also fluck ⁇  born : juen \n",
      "\n",
      "| epoch  10 |   100/  541 batches | lr 4.00 | loss  4.30 | ppl    73.35\n",
      "| epoch  10 |   200/  541 batches | lr 4.00 | loss  4.24 | ppl    69.15\n",
      "| epoch  10 |   300/  541 batches | lr 4.00 | loss  4.23 | ppl    68.95\n",
      "| epoch  10 |   400/  541 batches | lr 4.00 | loss  4.23 | ppl    68.61\n",
      "| epoch  10 |   500/  541 batches | lr 4.00 | loss  4.23 | ppl    68.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss 486.49 | valid ppl 19097880697772709823998668208982202991918776866379483887745626792670102683439467729005309802074861765804992443852171765027113025446951991470853031874051075691984850484525011594168586394073628554943874567831027712.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ger shoper  ⁇ unk ⁇  daysible compillestated with 00 ) offing emreasure incested in 0000 , commetly three , panding off \" chap groncah \n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(model, train_iter)\n",
    "    val_loss = evaluate(model, val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sample:\n",
      " id as well into polery mile on a bed for the 0 to ox on the video \" nazine st. ⁇ - ⁇  are a persanding apme raemeated at the minth at j \n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(model, bpemb.decode_ids, 50), '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0  ⁇ \n",
      "1 \n",
      "2 \n",
      "3 t\n",
      "4 a\n",
      "5 he\n",
      "6 in\n",
      "7 the\n",
      "8 er\n",
      "9 on\n",
      "10 s\n",
      "11 00\n",
      "12 re\n",
      "13 o\n",
      "14 c\n",
      "15 w\n",
      "16 an\n",
      "17 at\n",
      "18 ed\n",
      "19 en\n",
      "20 b\n",
      "21 f\n",
      "22 or\n",
      "23 is\n",
      "24 p\n",
      "25 it\n",
      "26 in\n",
      "27 of\n",
      "28 ar\n",
      "29 es\n",
      "30 al\n",
      "31 m\n",
      "32 an\n",
      "33 d\n",
      "34 and\n",
      "35 as\n",
      "36 ic\n",
      "37 ing\n",
      "38 ro\n",
      "39 00\n",
      "40 h\n",
      "41 ion\n",
      "42 to\n",
      "43 l\n",
      "44 ''\n",
      "45 ou\n",
      "46 il\n",
      "47 n\n",
      "48 el\n",
      "49 ent\n",
      "50 re\n",
      "51 g\n",
      "52 0000\n",
      "53 st\n",
      "54 le\n",
      "55 om\n",
      "56 am\n",
      "57 e\n",
      "58 th\n",
      "59 ol\n",
      "60 un\n",
      "61 ct\n",
      "62 *\n",
      "63 ad\n",
      "64 (\n",
      "65 et\n",
      "66 st\n",
      "67 ur\n",
      "68 iv\n",
      "69 ch\n",
      "70 us\n",
      "71 on\n",
      "72 for\n",
      "73 was\n",
      "74 ly\n",
      "75 id\n",
      "76 ation\n",
      "77 im\n",
      "78 ir\n",
      "79 as\n",
      "80 is\n",
      "81 ig\n",
      "82 ce\n",
      "83 he\n",
      "84 ut\n",
      "85 ot\n",
      "86 be\n",
      "87 ra\n",
      "88 ers\n",
      "89 ''\n",
      "90 ow\n",
      "91 v\n",
      "92 ith\n",
      "93 al\n",
      "94 em\n",
      "95 ter\n",
      "96 ay\n",
      "97 with\n",
      "98 ul\n",
      "99 j\n",
      "100 con\n",
      "101 by\n",
      "102 and\n",
      "103 wh\n",
      "104 r\n",
      "105 ist\n",
      "106 th\n",
      "107 it\n",
      "108 ver\n",
      "109 k\n",
      "110 rom\n",
      "111 ge\n",
      "112 ch\n",
      "113 os\n",
      "114 at\n",
      "115 her\n",
      "116 de\n",
      "117 um\n",
      "118 un\n",
      "119 com\n",
      "120 that\n",
      "121 0\n",
      "122 pro\n",
      "123 ac\n",
      "124 est\n",
      "125 se\n",
      "126 op\n",
      "127 or\n",
      "128 \"\n",
      "129 ri\n",
      "130 from\n",
      "131 od\n",
      "132 av\n",
      "133 ain\n",
      "134 ity\n",
      "135 res\n",
      "136 if\n",
      "137 oun\n",
      "138 up\n",
      "139 oc\n",
      "140 ne\n",
      "141 ill\n",
      "142 ies\n",
      "143 qu\n",
      "144 se\n",
      "145 art\n",
      "146 his\n",
      "147 ate\n",
      "148 ab\n",
      "149 ian\n",
      "150 pe\n",
      "151 ud\n",
      "152 ich\n",
      "153 0000\n",
      "154 are\n",
      "155 y\n",
      "156 ber\n",
      "157 ort\n",
      "158 all\n",
      "159 ex\n",
      "160 ard\n",
      "161 igh\n",
      "162 0000,\n",
      "163 ld\n",
      "164 ere\n",
      "165 ore\n",
      "166 ak\n",
      "167 ag\n",
      "168 ess\n",
      "169 ant\n",
      "170 ap\n",
      "171 ro\n",
      "172 le\n",
      "173 were\n",
      "174 ub\n",
      "175 ated\n",
      "176 sh\n",
      "177 ia\n",
      "178 ect\n",
      "179 ip\n",
      "180 ive\n",
      "181 pl\n",
      "182 ial\n",
      "183 en\n",
      "184 ar\n",
      "185 gh\n",
      "186 ary\n",
      "187 ment\n",
      "188 ov\n",
      "189 ie\n",
      "190 ine\n",
      "191 ast\n",
      "192 which\n",
      "193 ell\n",
      "194 der\n",
      "195 us\n",
      "196 ame\n",
      "197 ong\n",
      "198 ost\n",
      "199 ure\n",
      "200 ical\n",
      "201 our\n",
      "202 ish\n",
      "203 cl\n",
      "204 ire\n",
      "205 wor\n",
      "206 ust\n",
      "207 uc\n",
      "208 ther\n",
      "209 comp\n",
      "210 cl\n",
      "211 og\n",
      "212 ight\n",
      "213 af\n",
      "214 ),\n",
      "215 this\n",
      "216 not\n",
      "217 so\n",
      "218 mer\n",
      "219 000\n",
      "220 0000.\n",
      "221 ran\n",
      "222 had\n",
      "223 ad\n",
      "224 000\n",
      "225 ome\n",
      "226 whe\n",
      "227 ap\n",
      "228 the\n",
      "229 (0000\n",
      "230 sp\n",
      "231 ions\n",
      "232 new\n",
      "233 pt\n",
      "234 ous\n",
      "235 end\n",
      "236 man\n",
      "237 ear\n",
      "238 age\n",
      "239 mar\n",
      "240 ave\n",
      "241 ord\n",
      "242 pl\n",
      "243 per\n",
      "244 also\n",
      "245 ry\n",
      "246 .0\n",
      "247 are\n",
      "248 iz\n",
      "249 rit\n",
      "250 ass\n",
      "251 cont\n",
      "252 part\n",
      "253 ac\n",
      "254 ack\n",
      "255 fir\n",
      "256 int\n",
      "257 ser\n",
      "258 ign\n",
      "259 ab\n",
      "260 all\n",
      "261 has\n",
      "262 man\n",
      "263 ra\n",
      "264 sc\n",
      "265 ue\n",
      "266 first\n",
      "267 ide\n",
      "268 its\n",
      "269 ount\n",
      "270 ok\n",
      "271 res\n",
      "272 one\n",
      "273 wn\n",
      "274 ice\n",
      "275 per\n",
      "276 but\n",
      "277 comm\n",
      "278 their\n",
      "279 ater\n",
      "280 can\n",
      "281 tw\n",
      "282 port\n",
      "283 ult\n",
      "284 ib\n",
      "285 el\n",
      "286 col\n",
      "287 own\n",
      "288 ally\n",
      "289 '',\n",
      "290 –\n",
      "291 ond\n",
      "292 ition\n",
      "293 ations\n",
      "294 ).\n",
      "295 land\n",
      "296 year\n",
      "297 after\n",
      "298 ind\n",
      "299 ime\n",
      "300 ff\n",
      "301 act\n",
      "302 other\n",
      "303 ence\n",
      "304 ens\n",
      "305 who\n",
      "306 ric\n",
      "307 und\n",
      "308 ile\n",
      "309 have\n",
      "310 ren\n",
      "311 ang\n",
      "312 ree\n",
      "313 ould\n",
      "314 rec\n",
      "315 te\n",
      "316 hed\n",
      "317 im\n",
      "318 uct\n",
      "319 bec\n",
      "320 over\n",
      "321 clud\n",
      "322 su\n",
      "323 ach\n",
      "324 ag\n",
      "325 ory\n",
      "326 they\n",
      "327 ates\n",
      "328 ph\n",
      "329 inter\n",
      "330 out\n",
      "331 old\n",
      "332 dis\n",
      "333 ugh\n",
      "334 pr\n",
      "335 her\n",
      "336 two\n",
      "337 pres\n",
      "338 ,000\n",
      "339 ke\n",
      "340 fer\n",
      "341 bo\n",
      "342 includ\n",
      "343 for\n",
      "344 cent\n",
      "345 fe\n",
      "346 ks\n",
      "347 one\n",
      "348 ational\n",
      "349 uring\n",
      "350 ite\n",
      "351 there\n",
      "352 me\n",
      "353 des\n",
      "354 ely\n",
      "355 ade\n",
      "356 ace\n",
      "357 te\n",
      "358 sch\n",
      "359 ound\n",
      "360 been\n",
      "361 pre\n",
      "362 we\n",
      "363 amp\n",
      "364 ne\n",
      "365 up\n",
      "366 ents\n",
      "367 ited\n",
      "368 act\n",
      "369 ubl\n",
      "370 ts\n",
      "371 out\n",
      "372 aw\n",
      "373 jo\n",
      "374 .0%\n",
      "375 ood\n",
      "376 vel\n",
      "377 ev\n",
      "378 sy\n",
      "379 she\n",
      "380 over\n",
      "381 co\n",
      "382 play\n",
      "383 able\n",
      "384 ose\n",
      "385 ces\n",
      "386 i\n",
      "387 par\n",
      "388 ild\n",
      "389 gro\n",
      "390 ough\n",
      "391 when\n",
      "392 bl\n",
      "393 pol\n",
      "394 off\n",
      "395 ican\n",
      "396 qu\n",
      "397 ug\n",
      "398 ool\n",
      "399 ors\n",
      "400 under\n",
      "401 fl\n",
      "402 ph\n",
      "403 car\n",
      "404 tra\n",
      "405 spe\n",
      "406 gen\n",
      "407 more\n",
      "408 ased\n",
      "409 count\n",
      "410 les\n",
      "411 orn\n",
      "412 ied\n",
      "413 reg\n",
      "414 pe\n",
      "415 we\n",
      "416 ase\n",
      "417 iss\n",
      "418 cess\n",
      "419 00,\n",
      "420 ind\n",
      "421 ted\n",
      "422 during\n",
      "423 into\n",
      "424 some\n",
      "425 am\n",
      "426 sup\n",
      "427 sec\n",
      "428 eng\n",
      "429 ath\n",
      "430 war\n",
      "431 ance\n",
      "432 nor\n",
      "433 ivers\n",
      "434 most\n",
      "435 amer\n",
      "436 ed\n",
      "437 may\n",
      "438 ear\n",
      "439 time\n",
      "440 ons\n",
      "441 att\n",
      "442 ember\n",
      "443 urn\n",
      "444 bet\n",
      "445 mus\n",
      "446 cons\n",
      "447 kn\n",
      "448 ve\n",
      "449 ass\n",
      "450 tr\n",
      "451 -\n",
      "452 pop\n",
      "453 ress\n",
      "454 ma\n",
      "455 ward\n",
      "456 ake\n",
      "457 io\n",
      "458 sou\n",
      "459 ings\n",
      "460 ton\n",
      "461 loc\n",
      "462 als\n",
      "463 bro\n",
      "464 inc\n",
      "465 ouse\n",
      "466 ft\n",
      "467 no\n",
      "468 way\n",
      "469 ms\n",
      "470 city\n",
      "471 school\n",
      "472 years\n",
      "473 br\n",
      "474 hip\n",
      "475 bu\n",
      "476 serv\n",
      "477 gan\n",
      "478 gre\n",
      "479 char\n",
      "480 op\n",
      "481 red\n",
      "482 ail\n",
      "483 used\n",
      "484 oll\n",
      "485 em\n",
      "486 prod\n",
      "487 publ\n",
      "488 ious\n",
      "489 so\n",
      "490 ans\n",
      "491 only\n",
      "492 min\n",
      "493 ck\n",
      "494 sub\n",
      "495 him\n",
      "496 work\n",
      "497 would\n",
      "498 ular\n",
      "499 air\n",
      "500 fil\n",
      "501 world\n",
      "502 such\n",
      "503 aus\n",
      "504 rel\n",
      "505 will\n",
      "506 gr\n",
      "507 ep\n",
      "508 rans\n",
      "509 how\n",
      "510 ''.\n",
      "511 oth\n",
      "512 thro\n",
      "513 (0000)\n",
      "514 rib\n",
      "515 po\n",
      "516 ident\n",
      "517 stud\n",
      "518 ween\n",
      "519 acc\n",
      "520 fin\n",
      "521 mon\n",
      "522 form\n",
      "523 med\n",
      "524 olog\n",
      "525 high\n",
      "526 ick\n",
      "527 amil\n",
      "528 between\n",
      "529 cial\n",
      "530 ict\n",
      "531 house\n",
      "532 american\n",
      "533 pro\n",
      "534 gr\n",
      "535 univers\n",
      "536 many\n",
      "537 dist\n",
      "538 than\n",
      "539 ics\n",
      "540 trans\n",
      "541 ince\n",
      "542 ative\n",
      "543 lar\n",
      "544 through\n",
      "545 z\n",
      "546 ix\n",
      "547 ished\n",
      "548 tern\n",
      "549 do\n",
      "550 rem\n",
      "551 ten\n",
      "552 ating\n",
      "553 che\n",
      "554 rep\n",
      "555 ins\n",
      "556 num\n",
      "557 ter\n",
      "558 hn\n",
      "559 form\n",
      "560 gu\n",
      "561 gener\n",
      "562 ral\n",
      "563 ron\n",
      "564 prov\n",
      "565 ale\n",
      "566 ys\n",
      "567 int\n",
      "568 elect\n",
      "569 li\n",
      "570 ual\n",
      "571 where\n",
      "572 inst\n",
      "573 ret\n",
      "574 **\n",
      "575 overn\n",
      "576 mod\n",
      "577 ced\n",
      "578 00.0%\n",
      "579 iel\n",
      "580 later\n",
      "581 ities\n",
      "582 these\n",
      "583 ork\n",
      "584 att\n",
      "585 dr\n",
      "586 town\n",
      "587 sm\n",
      "588 ury\n",
      "589 govern\n",
      "590 states\n",
      "591 ific\n",
      "592 about\n",
      "593 united\n",
      "594 again\n",
      "595 mov\n",
      "596 ollow\n",
      "597 made\n",
      "598 def\n",
      "599 ement\n",
      "600 iver\n",
      "601 cap\n",
      "602 ier\n",
      "603 ision\n",
      "604 ject\n",
      "605 air\n",
      "606 met\n",
      "607 three\n",
      "608 while\n",
      "609 .00\n",
      "610 ction\n",
      "611 iet\n",
      "612 call\n",
      "613 u\n",
      "614 state\n",
      "615 –0000\n",
      "616 ower\n",
      "617 cre\n",
      "618 follow\n",
      "619 them\n",
      "620 art\n",
      "621 br\n",
      "622 stem\n",
      "623 ent\n",
      "624 found\n",
      "625 inv\n",
      "626 end\n",
      "627 0000)\n",
      "628 fore\n",
      "629 hist\n",
      "630 ann\n",
      "631 record\n",
      "632 ople\n",
      "633 pos\n",
      "634 north\n",
      "635 str\n",
      "636 son\n",
      "637 then\n",
      "638 national\n",
      "639 \".\n",
      "640 \",\n",
      "641 set\n",
      "642 ines\n",
      "643 well\n",
      "644 ah\n",
      "645 ames\n",
      "646 known\n",
      "647 ern\n",
      "648 az\n",
      "649 ever\n",
      "650 sur\n",
      "651 colle\n",
      "652 system\n",
      "653 ert\n",
      "654 famil\n",
      "655 ev\n",
      "656 bel\n",
      "657 second\n",
      "658 uary\n",
      "659 ife\n",
      "660 group\n",
      "661 south\n",
      "662 people\n",
      "663 university\n",
      "664 .\"\n",
      "665 velop\n",
      "666 ery\n",
      "667 ulation\n",
      "668 ob\n",
      "669 age\n",
      "670 film\n",
      "671 oper\n",
      "672 its\n",
      "673 iam\n",
      "674 pos\n",
      "675 add\n",
      "676 ss\n",
      "677 main\n",
      "678 became\n",
      "679 ism\n",
      "680 round\n",
      "681 led\n",
      "682 being\n",
      "683 however\n",
      "684 emb\n",
      "685 igin\n",
      "686 ather\n",
      "687 john\n",
      "688 ution\n",
      "689 sing\n",
      "690 fran\n",
      "691 including\n",
      "692 rele\n",
      "693 both\n",
      "694 alb\n",
      "695 develop\n",
      "696 ise\n",
      "697 day\n",
      "698 dep\n",
      "699 uction\n",
      "700 sim\n",
      "701 show\n",
      "702 lead\n",
      "703 dire\n",
      "704 oy\n",
      "705 est\n",
      "706 cc\n",
      "707 cy\n",
      "708 ature\n",
      "709 ived\n",
      "710 ley\n",
      "711 ob\n",
      "712 fl\n",
      "713 ield\n",
      "714 ures\n",
      "715 sa\n",
      "716 ger\n",
      "717 aut\n",
      "718 con\n",
      "719 four\n",
      "720 dif\n",
      "721 ved\n",
      "722 soc\n",
      "723 sever\n",
      "724 origin\n",
      "725 ty\n",
      "726 const\n",
      "727 appe\n",
      "728 ute\n",
      "729 ually\n",
      "730 other\n",
      "731 dec\n",
      "732 ason\n",
      "733 arm\n",
      "734 var\n",
      "735 bar\n",
      "736 ger\n",
      "737 nov\n",
      "738 writ\n",
      "739 sh\n",
      "740 series\n",
      "741 compan\n",
      "742 brit\n",
      "743 cor\n",
      "744 iving\n",
      "745 before\n",
      "746 ists\n",
      "747 ock\n",
      "748 ik\n",
      "749 old\n",
      "750 #\n",
      "751 ause\n",
      "752 rev\n",
      "753 iew\n",
      "754 own\n",
      "755 area\n",
      "756 any\n",
      "757 team\n",
      "758 number\n",
      "759 min\n",
      "760 enn\n",
      "761 ge\n",
      "762 sl\n",
      "763 king\n",
      "764 sign\n",
      "765 elf\n",
      "766 use\n",
      "767 til\n",
      "768 rol\n",
      "769 long\n",
      "770 la\n",
      "771 ient\n",
      "772 cept\n",
      "773 ank\n",
      "774 name\n",
      "775 music\n",
      "776 ext\n",
      "777 since\n",
      "778 ized\n",
      "779 mil\n",
      "780 inn\n",
      "781 $\n",
      "782 county\n",
      "783 *''\n",
      "784 government\n",
      "785 commun\n",
      "786 intern\n",
      "787 called\n",
      "788 direct\n",
      "789 maj\n",
      "790 ract\n",
      "791 uro\n",
      "792 population\n",
      "793 exp\n",
      "794 cro\n",
      "795 album\n",
      "796 until\n",
      "797 ales\n",
      "798 urch\n",
      "799 cr\n",
      "800 red\n",
      "801 uch\n",
      "802 public\n",
      "803 uss\n",
      "804 pers\n",
      "805 ired\n",
      "806 early\n",
      "807 av\n",
      "808 design\n",
      "809 har\n",
      "810 back\n",
      "811 0,\n",
      "812 several\n",
      "813 ars\n",
      "814 ann\n",
      "815 offic\n",
      "816 land\n",
      "817 imes\n",
      "818 line\n",
      "819 german\n",
      "820 ina\n",
      "821 ful\n",
      "822 go\n",
      "823 sw\n",
      "824 ered\n",
      "825 amed\n",
      "826 mat\n",
      "827 ined\n",
      "828 west\n",
      "829 ible\n",
      "830 ty\n",
      "831 major\n",
      "832 against\n",
      "833 jun\n",
      "834 lish\n",
      "835 ared\n",
      "836 ained\n",
      "837 book\n",
      "838 cur\n",
      "839 comple\n",
      "840 port\n",
      "841 ural\n",
      "842 ull\n",
      "843 get\n",
      "844 ically\n",
      "845 contin\n",
      "846 to\n",
      "847 open\n",
      "848 (0000–0000\n",
      "849 child\n",
      "850 eff\n",
      "851 ives\n",
      "852 val\n",
      "853 ional\n",
      "854 suc\n",
      "855 gram\n",
      "856 cour\n",
      "857 ility\n",
      "858 dem\n",
      "859 song\n",
      "860 ium\n",
      "861 ange\n",
      "862 ting\n",
      "863 arch\n",
      "864 uced\n",
      "865 ever\n",
      "866 family\n",
      "867 ket\n",
      "868 ke\n",
      "869 ple\n",
      "870 same\n",
      "871 ister\n",
      "872 law\n",
      "873 ues\n",
      "874 oot\n",
      "875 if\n",
      "876 mill\n",
      "877 support\n",
      "878 class\n",
      "879 gl\n",
      "880 ball\n",
      "881 '\n",
      "882 each\n",
      "883 band\n",
      "884 0,000\n",
      "885 cal\n",
      "886 vers\n",
      "887 program\n",
      "888 run\n",
      "889 differ\n",
      "890 els\n",
      "891 ried\n",
      "892 near\n",
      "893 game\n",
      "894 bus\n",
      "895 award\n",
      "896 .\n",
      "897 che\n",
      "898 polit\n",
      "899 div\n",
      "900 ir\n",
      "901 company\n",
      "902 leg\n",
      "903 ird\n",
      "904 eter\n",
      "905 bo\n",
      "906 raph\n",
      "907 rist\n",
      "908 hel\n",
      "909 organ\n",
      "910 ash\n",
      "911 small\n",
      "912 \n",
      "913 e\n",
      "914 a\n",
      "915 t\n",
      "916 i\n",
      "917 n\n",
      "918 o\n",
      "919 r\n",
      "920 s\n",
      "921 h\n",
      "922 l\n",
      "923 d\n",
      "924 c\n",
      "925 0\n",
      "926 u\n",
      "927 m\n",
      "928 f\n",
      "929 p\n",
      "930 g\n",
      "931 w\n",
      "932 b\n",
      "933 y\n",
      "934 ,\n",
      "935 .\n",
      "936 v\n",
      "937 '\n",
      "938 k\n",
      "939 *\n",
      "940 -\n",
      "941 )\n",
      "942 (\n",
      "943 j\n",
      "944 \"\n",
      "945 x\n",
      "946 z\n",
      "947 q\n",
      "948 :\n",
      "949 –\n",
      "950 %\n",
      "951 ;\n",
      "952 /\n",
      "953 é\n",
      "954 #\n",
      "955 $\n",
      "956 —\n",
      "957 &\n",
      "958 ’\n",
      "959 |\n",
      "960 !\n",
      "961 á\n",
      "962 =\n",
      "963 ó\n",
      "964 ö\n",
      "965 ü\n",
      "966 í\n",
      "967 ?\n",
      "968 ”\n",
      "969 “\n",
      "970 +\n",
      "971 ō\n",
      "972 ä\n",
      "973 2\n",
      "974 è\n",
      "975 ā\n",
      "976 _\n",
      "977 £\n",
      "978 ç\n",
      "979 ł\n",
      "980 š\n",
      "981 °\n",
      "982 ć\n",
      "983 а\n",
      "984 ñ\n",
      "985 о\n",
      "986 ø\n",
      "987 ú\n",
      "988 ×\n",
      "989 ã\n",
      "990 и\n",
      "991 č\n",
      "992 е\n",
      "993 −\n",
      "994 ‘\n",
      "995 ū\n",
      "996 â\n",
      "997 н\n",
      "998 α\n",
      "999 à\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# printing all avaliable syllables\n",
    "for i in range(1000):\n",
    "    print(i, bpemb.decode_ids([i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}